---
title: "Survey Planning with svyplan"
output: rmarkdown::html_vignette
bibliography: references.bib
link-citations: true
vignette: >
  %\VignetteIndexEntry{Survey Planning with svyplan}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message = FALSE, echo = FALSE}
library(samplyr)
library(svyplan)
library(dplyr)
```

## Overview

Before drawing a sample, you need to answer two questions: how many units to select, and how to divide them across strata. The `svyplan` package provides tools for both. `samplyr` integrates with `svyplan` so that planning results flow directly into `draw()` and `execute()`.

The three packages form a pipeline:

- **svyplan** determines sample sizes, evaluates precision, and computes strata boundaries (planning).
- **samplyr** specifies and executes the sampling design (selection).
- **sondage** provides the low-level sampling algorithms (engine).

This vignette uses the `ken_enterprises` dataset, a synthetic frame of 6,823 Kenyan establishments with revenue, employee counts, and export status.

```{r}
data(ken_enterprises)
glimpse(ken_enterprises)
```

## Sample Size for a Proportion

Suppose we want to estimate the proportion of exporting firms. We know from the census that about 17% of firms export. We want a margin of error of 3 percentage points at 95% confidence.

```{r}
n_export <- n_prop(p = 0.17, moe = 0.03)
n_export
```

The result is a `svyplan_n` object. You can pass it directly to `draw()`:

```{r}
samp <- sampling_design() |>
  draw(n = n_export) |>
  execute(ken_enterprises, seed = 1)
nrow(samp)
```

## Sample Size for a Mean

We also want to estimate mean revenue with a margin of error of 30 million KES. We compute the population variance from the frame:

```{r}
rev_var <- var(ken_enterprises$revenue_millions)
n_rev <- n_mean(var = rev_var, moe = 30)
n_rev
```

## Multiple Indicators

In practice, a survey measures several things at once. `n_multi()` takes the binding constraint across indicators:

```{r}
targets <- data.frame(
  indicator = c("exporter_rate", "mean_revenue"),
  p = c(0.17, NA),
  var = c(NA, rev_var),
  moe = c(0.03, 30)
)
n_survey <- n_multi(targets)
n_survey
```

The sample size is driven by whichever indicator requires the most units. Pass it to `draw()` like any other sample size:

```{r}
samp <- sampling_design() |>
  draw(n = n_survey) |>
  execute(ken_enterprises, seed = 2)
nrow(samp)
```

## Power Analysis

If the goal is to detect a difference (for example, comparing exporter rates between two regions), `power_prop()` gives the required per-group sample size:

```{r}
n_power <- power_prop(p1 = 0.17, p2 = 0.25, power = 0.8)
n_power
```

This also works directly with `draw()`:

```{r}
samp <- sampling_design() |>
  draw(n = n_power) |>
  execute(ken_enterprises, seed = 3)
nrow(samp)
```

## Response Rate Adjustment

All svyplan functions accept a `resp_rate` parameter. The sample size is inflated by `1 / resp_rate` to account for expected non-response:

```{r}
n_prop(p = 0.17, moe = 0.03, resp_rate = 0.85)
```

The net sample size (after non-response) matches the original target. This works in `n_mean()`, `n_multi()`, `n_cluster()`, and `power_*()` functions as well.

## Precision Analysis

Given a fixed sample size, how precise will the estimates be? The `prec_*()` functions are the inverse of `n_*()`:

```{r}
prec_prop(p = 0.17, n = 600)
```

```{r}
prec_mean(var = rev_var, n = 300, mu = mean(ken_enterprises$revenue_millions))
```

### Round-trip between size and precision

All `n_*()` and `prec_*()` functions are S3 generics. Pass a precision result to `n_*()` to recover the sample size, or a sample size result to `prec_*()` to compute the achieved precision:

```{r}
s <- n_prop(p = 0.17, moe = 0.03)
p <- prec_prop(s)
p

# Recover the original n
n_prop(p)
```

You can override a parameter on the way back:

```{r}
n_prop(p, cv = 0.10)
```

### Confidence intervals

`confint()` extracts the expected confidence interval from any sample size or precision object:

```{r}
confint(s)
confint(p, level = 0.99)
```

## Sensitivity Analysis

How sensitive is a sample size to assumptions about the design effect or response rate? The `predict()` method evaluates any svyplan result at new parameter combinations:

```{r}
x <- n_prop(p = 0.17, moe = 0.03, deff = 1.5)
predict(x, expand.grid(
  deff = c(1.0, 1.5, 2.0, 2.5),
  resp_rate = c(0.8, 0.9, 1.0)
))
```

This works for all svyplan object types: sample size, precision, cluster, and power.

## Multistage Cluster Planning

For cluster designs, `n_cluster()` finds the optimal allocation across stages given costs and the degree of clustering.

### Variance components

If a frame with cluster identifiers is available, `varcomp()` estimates the between-cluster and within-cluster variance components:

```{r}
vc <- varcomp(
  revenue_millions ~ county,
  data = ken_enterprises
)
vc
```

The `delta` value (measure of homogeneity) feeds directly into `n_cluster()` and `design_effect()`.

### Optimal allocation

```{r}
# Minimize cost to achieve CV = 0.05
n_cluster(cost = c(500, 50), delta = vc, cv = 0.05)

# Maximize precision within a budget
n_cluster(cost = c(500, 50), delta = vc, budget = 100000)
```

The total sample size from `n_cluster()` can be passed to `draw()`:

```{r}
cl <- n_cluster(cost = c(500, 50), delta = vc, budget = 100000)
samp <- sampling_design() |>
  draw(n = cl) |>
  execute(ken_enterprises, seed = 5)
nrow(samp)
```

## Stratification Boundaries

Enterprise surveys typically stratify by size. Rather than using predefined size classes, `strata_bound()` finds optimal boundaries on a continuous variable. The cumulative square root of frequency method minimizes the coefficient of variation for a given number of strata and total sample size:

```{r}
bounds <- strata_bound(
  ken_enterprises$revenue_millions,
  n_strata = 4,
  method = "cumrootf",
  cv = 0.05
)
bounds
```

The result includes jointly optimized boundaries and stratum sample sizes. The `predict()` method assigns each frame unit to a stratum:

```{r}
labels <- paste0("S", 1:4)
ken_enterprises$rev_stratum <- predict(
  bounds,
  ken_enterprises$revenue_millions,
  labels = labels
)
table(ken_enterprises$rev_stratum)
```

Use the stratum allocations from `strata_bound()` directly as a named vector for `draw()`. This preserves the joint optimization between boundaries and allocation:

```{r}
n_alloc <- setNames(bounds$strata$n_h, labels)
n_alloc
```

```{r}
samp <- sampling_design() |>
  stratify_by(rev_stratum) |>
  draw(n = n_alloc) |>
  execute(ken_enterprises, seed = 4)
samp |>
  count(rev_stratum, name = "n_sampled")
```

Note that the boundaries and allocation are coupled. The `cumrootf` method optimizes them together to achieve the target CV. Using `strata_bound()` for boundaries but a different allocation in `samplyr` (for example, proportional instead of Neyman) might break this optimality. If you need a different allocation method, compute boundaries and allocation separately.

## Design Effects After Sampling

After executing a design, you can evaluate its efficiency using `design_effect()` and `effective_n()`. These are re-exported from
`svyplan` and work directly on `tbl_sample` objects.

### Kish Design Effect

The Kish design effect measures the loss of precision due to unequal weights. It equals 1 for self-weighting designs:

```{r}
design_effect(samp)
effective_n(samp)
```

The effective sample size is the number of observations an SRS would need to achieve the same precision.

### Spencer Method

The Spencer method accounts for the relationship between the outcome variable and the selection probabilities. Pass the outcome as a bare column name. The selection probabilities are extracted automatically
from `.weight_1`:

```{r}
design_effect(samp, y = revenue_millions, method = "spencer")
```

### Henry Method

The Henry method also accounts for a calibration covariate. Both `y` and `x_cal` are column names in the sample:

```{r}
design_effect(samp, y = revenue_millions, x_cal = employees,
              method = "henry")
```

### Chen-Rust Decomposition

For stratified or clustered designs, the Chen-Rust (CR) method decomposes the design effect into weighting (`deff_w`), clustering (`deff_c`), and stratification (`deff_s`) components:

```{r}
cr <- design_effect(samp, y = revenue_millions, method = "cr")
cr$strata
cr$overall
```

The `tbl_sample` method extracts stratification and clustering variables from the stored design metadata. You only need to supply the outcome.

### Cluster Planning

Before collecting data, you can anticipate the design effect for a cluster design using the intraclass correlation coefficient (ICC) and
cluster size. This uses `svyplan::design_effect()` directly (not through a `tbl_sample`):

```{r}
design_effect(delta = 0.05, m = 10)
design_effect(delta = 0.15, m = 10)
```

With an ICC of 0.05 and 10 units per cluster, the design effect is 1.45. An ICC of 0.15 raises it to 2.35. This helps decide the number of clusters and units per cluster during the planning stage.

## Summary

The typical planning workflow can be:

1. Determine sample size with `n_prop()`, `n_mean()`, `n_multi()`, or
   `power_prop()` / `power_mean()`.
2. Evaluate precision at the chosen *n* with `prec_prop()`, `prec_mean()`.
3. Run sensitivity analysis with `predict()` to check robustness.
4. Optionally create stratification boundaries with `strata_bound()` and
   assign strata with `predict()`.
5. For cluster designs, estimate variance components with `varcomp()` and
   optimize the allocation with `n_cluster()`.
6. Pass the sample size to `draw()` and execute the design.
7. Evaluate the realized design with `design_effect()` and `effective_n()`.

All `svyplan` sample size objects (`svyplan_n`, `svyplan_power`,
`svyplan_cluster`) work directly with `draw()`, so the planning and
execution steps connect without manual conversion.
